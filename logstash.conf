input {
  file {
    path => "<path>extract*.csv"
    type => "tfl"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {

  uuid {
    target => "@uuid"
    overwrite => true
  }

  # generate fingerprint to prevent duplication
  fingerprint {
    source => ["message"]
    target => "fingerprint"
    key => "78787878"
    method => "SHA1"
    concatenate_sources => true
  }


  if [type] == "tfl" {

    # drop empty line
    grok {
      match => {message =>"^\s*$" }
      add_tag => "drop"
    }


    # drop headers
    grok {
      match => { message => "^Date," }
      add_tag => "drop"
    }

    if "drop" in [tags] {
      drop { }
    }

    # parse csv
    csv {
      columns => ["Date", "Start Time", "End Time", "Journey/Action", "Charge", "Credit", "Balance", "Note" ]
      separator => ","
    }

    # transform date + starttime + endtime to datetime, then remove those fields
    # convert numeric fields to float
    mutate {
      add_field => [ "DateStartTime", "%{Date}T%{Start Time}" ]
      add_field => [ "DateEndTime", "%{Date}T%{End Time}" ]
      remove_field => ["Date", "Start Time", "End Time" ]
      replace => ["DateEndTime", "%{End Time}"]
      convert => { "Charge" => "float" }
      convert => { "Credit" => "float" }
      convert => { "Balance" => "float" }
    }

    date {
      match => [ "DateStartTime", "dd-MMM-YYYY'T'HH:mm"]
      timezone => "UTC"
      target => "@timestamp"
    }

  }
}

output {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "%{type}"
      # prevent duplication
      document_id => "%{fingerprint}"
    }
    stdout {
      codec => rubydebug
    }
}


